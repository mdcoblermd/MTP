{"cells":[{"cell_type":"code","execution_count":null,"id":"332b9791","metadata":{"id":"332b9791"},"outputs":[],"source":["##install and import necessary modules\n","##this code was originally designed and run in google colab\n","##use outside of colab may require modification\n","##if using colab, you may need to restart your runtime after installing modules,\n","##depending on enviornment at time of code running.\n","\n","!pip install scikit-learn==1.5.2\n","!pip install tensorflow==2.12.1\n","!pip install xgboost==2.0.2\n","import sys\n","import sklearn\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import xgboost as xgb\n","from google.colab import drive\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import roc_auc_score, f1_score, roc_curve, auc, precision_recall_curve, recall_score, confusion_matrix, brier_score_loss\n","from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.utils import resample\n","\n","print(\"Python version:\", sys.version)\n","print(\"scikit-learn version:\", sklearn.__version__)\n","print(\"XGBoost version:\", xgb.__version__)"]},{"cell_type":"code","execution_count":null,"id":"Ik3aE1RPVETF","metadata":{"id":"Ik3aE1RPVETF"},"outputs":[],"source":["##import your dataset\n","\n","##mount google drive if using in colab. Replace <MOUNT_POINT> with the directory where you want to mount the drive (e.g., /content/drive).\n","drive.mount('<MOUNT_POINT>')\n","\n","# Replace <YOUR_FILE_PATH> with the actual path inside your Google Drive (e.g., My Drive/FileNameHere).\n","file_path = '<MOUNT_POINT>/<YOUR_FILE_PATH>.csv'"]},{"cell_type":"code","execution_count":null,"id":"a9e8e1be","metadata":{"id":"a9e8e1be"},"outputs":[],"source":["##import data and specify missing values\n","raw_data = pd.read_csv(file_path, na_values=['NA', 'N/A', 'NULL', ' ', '', '-99', '-98', '-99.0', '-99.00', '-98.0', '-98.00', 'NaN'])\n","\n","##filter data such that we only include non-missing data, and pts transfused more than unitsCutoff (10 for UMT)\n","unitsCutoff=10.0\n","data = raw_data[raw_data['UnitsWBandRBC'].notna() & (raw_data['UnitsWBandRBC'] >= unitsCutoff)]\n","\n","##reset indices of the df\n","data.reset_index(drop=True, inplace=True)"]},{"cell_type":"code","execution_count":null,"id":"522c0982","metadata":{"id":"522c0982"},"outputs":[],"source":["##check dataframe to ensure it appears as it should\n","data.head()"]},{"cell_type":"code","execution_count":null,"id":"ab349743","metadata":{"id":"ab349743"},"outputs":[],"source":["##check for missing data\n","data.isnull().sum(axis=0)"]},{"cell_type":"code","execution_count":null,"id":"e0ad2ed6","metadata":{"id":"e0ad2ed6"},"outputs":[],"source":["##create a dataframe of all complications and variables not included in this model version.  We can remove all of these from the X data set and pick one to be\n","#our Y dataset\n","\n","complications_df=pd.DataFrame()\n","complications_list= ['HC_CLABSI', 'HC_DEEPSSI', 'HC_DVTHROMBOSIS', 'HC_ALCOHOLWITHDRAWAL', 'HC_CARDARREST', 'HC_CAUTI',\n","                    'HC_EMBOLISM', 'HC_EXTREMITYCS', 'HC_INTUBATION', 'HC_KIDNEY', 'HC_MI', 'HC_ORGANSPACESSI',\n","                    'HC_OSTEOMYELITIS', 'HC_RESPIRATORY', 'HC_RETURNOR', 'HC_SEPSIS', 'HC_STROKECVA', 'HC_SUPERFICIALINCISIONSSI',\n","                    'HC_PRESSUREULCER', 'HC_UNPLANNEDICU', 'HC_VAPNEUMONIA',\n","                    'MORTALITY', 'Mortality3Hr', 'Mortality6Hr', 'Mortality12Hr', 'Mortality24Hr', 'Mortality14d', 'Mortality30d',\n","                    'HOSPDISCHARGEDISPOSITION', 'EDDISCHARGEDISPOSITION', 'EDDISCHARGEHRS', 'EDDISCHARGEDAYS', 'FINALDISCHARGEHRS', 'FINALDISCHARGEDAYS',\n","                    'TOTALICULOS', 'TOTALVENTDAYS', 'FacilityKey',\n","                    'WITHDRAWALLST', 'WITHDRAWALLSTHRS', 'WITHDRAWALLSTDAYS',\n","                    'VTEPROPHYLAXISDAYS', 'VTEPROPHYLAXISHRS', 'VTEPROPHYLAXISTYPE',\n","                    'ESLIVER', 'ESPELVIS', 'ESSPLEEN', 'ESKIDNEY', 'ESRETROPERI', 'ESVASCULAR', 'ESOTHER', 'ES_UK', 'ES_NA', 'ANGIOGRAPHYHRS', 'ANGIOGRAPHYDAYS',\n","                    'ISS_05', 'AIS_FACE', 'AIS_NECK', 'AIS_HEAD', 'AIS_THORAX', 'AIS_ABDOMEN', 'AIS_SPINE', 'AIS_UPPEREX', 'AIS_LOWEREX', 'AIS_SKIN', 'AIS_OTHER',\n","                    'ICPEVDRAIN', 'ICPPARENCH', 'ICPO2MONITOR', 'ICPJVBULB', 'ICPNONE', 'ICP_NA', 'ICP_UK',\n","                    'DRGSCR_AMPHETAMINE', 'DRGSCR_BARBITURATE', 'DRGSCR_BENZODIAZEPINES', 'DRGSCR_COCAINE', 'DRGSCR_METHAMPHETAMINE',\n","                    'DRGSCR_ECSTASY', 'DRGSCR_METHADONE', 'DRGSCR_OPIOID', 'DRGSCR_OXYCODONE', 'DRGSCR_PHENCYCLIDINE', 'DRGSCR_TRICYCLICDEPRESS',\n","                    'DRGSCR_CANNABINOID', 'DRGSCR_OTHER', 'DRGSCR_NONE', 'DRGSCR_NOTTESTED', 'DRGSCR_UK', 'DRGSCR_NA', 'ALCOHOLSCREEN', \"ALCOHOLSCREENRESULT\",\n","                     'CC_ADHD', 'CC_ADLC', 'CC_ALCOHOLISM', 'CC_ANGINAPECTORIS', 'CC_ANTICOAGULANT', 'CC_BLEEDING', 'CC_CHEMO', 'CC_CIRRHOSIS', 'CC_CONGENITAL',\n","                     'CC_COPD', 'CC_CVA', 'CC_DEMENTIA', 'CC_DIABETES', 'CC_DISCANCER', 'CC_FUNCTIONAL', 'CC_CHF', 'CC_HYPERTENSION', 'CC_MI', 'CC_PAD',\n","                     'CC_PREMATURITY', 'CC_MENTALPERSONALITY', 'CC_RENAL', 'CC_SMOKING', 'CC_STEROID', 'CC_SUBSTANCEABUSE', 'mFI'\n","                     ,'IntracranialVascularInjury', 'BrainStemInjury', 'EDH', 'SAH', 'SDH', 'IPH', 'SkullFx', 'DAI', 'NeckVascularInjury', 'ThoracicVascularInjury'\n","                     ,'AeroDigestiveInjury', 'CardiacInjury', 'LungInjury', 'AbdominalVascular', 'RibFx', 'KidneyInjury', 'StomachInjury', 'SpleenInjury', 'UroGenInternalInjury'\n","                     , 'SCI', 'SpineFx', 'UEAmputation', 'UEVascularInjury', 'UELongBoneFx', 'LEVascularInjury', 'PelvicFx', 'LEAmputation', 'PancreasInjury'\n","                     , 'LELongBoneFx', 'LiverInjury', 'ColorectalInjury', 'SmallBowelInjury', 'NumberOfInjuries'\n","                     , 'AngioInFour', 'HmgCtrlSurgInFour'\n","                     , 'missingGCS', 'missingAge', 'missingSex', 'missingType', 'missingSBP', 'missingHR', 'missingRR', 'missingPulseOx', 'missingHeight', 'missingWeight'\n","                     , 'TotalDeathsFacility', 'FacilityKey', 'TM_NA', 'TM_UK', 'PROTDEV_NA', 'PROTDEV_UK', 'VPO_NA', 'VPO_UK'\n","                    ]\n","for c in complications_list:\n","    complications_df[c] = data[c]\n","complications_df"]},{"cell_type":"code","execution_count":null,"id":"8650e14e","metadata":{"id":"8650e14e"},"outputs":[],"source":["##this is where we choose our outcome variable, in this case, 'Mortality6Hr', and move it to a separate dataframe\n","Y_data = pd.DataFrame()\n","Y_data['MORTALITY'] = data['Mortality6Hr']\n","Y_data"]},{"cell_type":"code","execution_count":null,"id":"75be5241","metadata":{"id":"75be5241"},"outputs":[],"source":["##clean Y_data by replacing \"Yes\" and \"No\" vcalues with 0's and 1's\n","\n","Y_data['MORTALITY'] = Y_data['MORTALITY'].replace({'Yes': 1, 'No': 0})\n","Y_data"]},{"cell_type":"code","execution_count":null,"id":"90944eb8","metadata":{"id":"90944eb8"},"outputs":[],"source":["##remove all unwanted variables as defined above from the input space\n","X_data = data.drop(columns=complications_list)\n","X_data.shape"]},{"cell_type":"code","execution_count":null,"id":"eh992q4ujyp7","metadata":{"id":"eh992q4ujyp7"},"outputs":[],"source":["##need to remove any cases with missing data for our outcome variable\n","Missing_Y = Y_data.isnull().sum(axis=0)\n","Missing_Y"]},{"cell_type":"code","execution_count":null,"id":"73arQ_SPj1Iy","metadata":{"id":"73arQ_SPj1Iy"},"outputs":[],"source":["##here we find which rows in Y have missing values\n","bad_row_index_list=[]\n","for index, row in Y_data.iterrows():\n","    n_missings=row.isnull().sum()\n","    if n_missings>0:\n","        bad_row_index_list.append(index)\n","bad_row_index_list"]},{"cell_type":"code","execution_count":null,"id":"b9270983","metadata":{"id":"b9270983"},"outputs":[],"source":["##now remove the bad rows in Y\n","Y_clean = Y_data.drop(bad_row_index_list, axis=0)\n","Y_clean"]},{"cell_type":"code","execution_count":null,"id":"AUTIminQl6Lh","metadata":{"id":"AUTIminQl6Lh"},"outputs":[],"source":["##ensure all cases with missing values for the outcome have been dropped\n","Missing_Y_clean = Y_clean.isnull().sum(axis=0)\n","Missing_Y_clean"]},{"cell_type":"code","execution_count":null,"id":"M4GwsrAqj_r2","metadata":{"id":"M4GwsrAqj_r2"},"outputs":[],"source":["##and remove bad rows in X\n","X_data=X_data.drop(bad_row_index_list, axis=0)"]},{"cell_type":"code","execution_count":null,"id":"088ce1ba","metadata":{"id":"088ce1ba"},"outputs":[],"source":["##check which variables in the input space have missing variables\n","Missing = X_data.isnull().sum(axis=0)\n","Missing[Missing>0]"]},{"cell_type":"code","execution_count":null,"id":"qaR1MA4zQX5n","metadata":{"id":"qaR1MA4zQX5n"},"outputs":[],"source":["##order variables with missing data by percentage\n","data_missing = (X_data.isnull().sum(axis=0)/X_data.shape[0]) * 100\n","data_missing"]},{"cell_type":"code","execution_count":null,"id":"FtR5sBNwQaDW","metadata":{"id":"FtR5sBNwQaDW"},"outputs":[],"source":["##display variables withOUT mising data\n","data_missing[data_missing == 0].index"]},{"cell_type":"code","execution_count":null,"id":"jCvUHe_RQbym","metadata":{"id":"jCvUHe_RQbym"},"outputs":[],"source":["#remove the good columns (no missing values) from data_missing\n","data_missing = data_missing.drop(data_missing[data_missing == 0].index)\n","data_missing"]},{"cell_type":"code","execution_count":null,"id":"5qbwbipXQdnA","metadata":{"id":"5qbwbipXQdnA"},"outputs":[],"source":["#sort this in ascending order\n","data_missing = data_missing.sort_values(ascending=False)\n","data_missing"]},{"cell_type":"code","execution_count":null,"id":"pvKzlMEpQfcT","metadata":{"id":"pvKzlMEpQfcT"},"outputs":[],"source":["##prepare to drop variables with >50% missing values.\n","dropCutoff=50\n","bad_column_names = data_missing[data_missing >=dropCutoff].index\n","bad_column_names"]},{"cell_type":"code","execution_count":null,"id":"uMsxdfhfQg2M","metadata":{"id":"uMsxdfhfQg2M"},"outputs":[],"source":["##actually drop bad variables\n","X_data_new=X_data.drop(columns=bad_column_names, axis=1)\n","\n","##check for which variables still have missing data (<50% missing values)\n","Missing = X_data_new.isnull().sum(axis=0)\n","Missing[Missing>0]"]},{"cell_type":"code","execution_count":null,"id":"HIBPbQv8QifC","metadata":{"id":"HIBPbQv8QifC"},"outputs":[],"source":["#check for columns with less than 50% missing that need to be cleaned\n","to_be_cleaned_column_names = data_missing[data_missing <50].index\n","to_be_cleaned_column_names"]},{"cell_type":"code","execution_count":null,"id":"b5ec271e","metadata":{"id":"b5ec271e"},"outputs":[],"source":["##perform median imputation for continuous variable and mode imputation for categorical\n","for c in to_be_cleaned_column_names:\n","    v=X_data_new[c]#get values in this column\n","    v_valid=v[~v.isnull()] # get valid values\n","    if X_data_new[c].dtype == np.dtype('O'): # non-numeric values\n","        X_data_new[c]=X_data_new[c].fillna(v.value_counts().index[0]).astype(object) # the most frequent category\n","    else: # numeric\n","        X_data_new[c]=X_data_new[c].fillna(v_valid.median()) #replace nan with median value"]},{"cell_type":"code","execution_count":null,"id":"31a5eafa","metadata":{"id":"31a5eafa"},"outputs":[],"source":["##confirm no more missing data in imput space\n","X_data_new.isnull().sum().sum()"]},{"cell_type":"code","execution_count":null,"id":"0a109d01","metadata":{"id":"0a109d01"},"outputs":[],"source":["##verify cleaned dataframe appears as intended\n","X_data_new.head()"]},{"cell_type":"code","execution_count":null,"id":"aHrlRyyIkk5V","metadata":{"id":"aHrlRyyIkk5V"},"outputs":[],"source":["##remove any additional variables necessary\n","## Remove the \"RACE\" and \"TRANSPORTMODE\" columns, as these are composite varibles that have already been 1 hot encoded\n","##primarypaymentmethod unlikely to be available on arrival\n","##remove any existing transfusion related variables, as that is not available on arrival\n","##remove any hemorrhage control surgery variables, as not available on arrival\n","columns_to_remove = ['RACE', 'TRANSPORTMODE', 'UnitsWBandRBC','PRIMARYMETHODPAYMENT', 'PLASMAUNITS', 'RoundedUnitsWBandRBC', 'TransfusionRange',\n","                    'BLOODBINARY', 'PLASMABINARY', 'PLATELETSBINARY', 'CRYOBINARY', 'WHOLEBLOODBINARY', 'HMRRHGCTRLSURGHRS', 'HMRRHGCTRLSURGDAYS',\n","                    'Units5to9']\n","X_data_new = X_data_new.drop(columns=columns_to_remove, errors='ignore')"]},{"cell_type":"code","execution_count":null,"id":"2727c717","metadata":{"id":"2727c717"},"outputs":[],"source":["##first we will convert No's and Yes's to 0's and 1's to minimize the amount of double variables (want to avoid Yes/Nos being converted to 1-hot variables)\n","##want code to be reusable between different populations of input data.  Not every population will have all of these variables\n","##Therefore, will do everything within separate try/except blocks\n","try:\n","    X_data_new= X_data_new.replace({True: 1, 'Yes': 1, \"Female\": 1, False: 0, 'No': 0, \"Male\": 0})\n","except:\n","    pass\n","try:\n","    X_data_new['ETHNICITY'] = X_data_new['ETHNICITY'].replace({'Hispanic or Latino': 1, 'Not Hispanic or Latino': 0})\n","except:\n","    pass\n","try:\n","    X_data_new['EMSGCSEYE'] = X_data_new['EMSGCSEYE'].replace({'None': 1, 'To pressure': 2, 'To sound': 3,\n","                                                               'Spontaneous': 4})\n","except:\n","    pass\n","try:\n","    X_data_new['GCSEYE'] = X_data_new['GCSEYE'].replace({'None': 1, 'To pressure': 2, 'To sound': 3, 'Spontaneous': 4})\n","except:\n","    pass\n","try:\n","    X_data_new['EMSGCSVERBAL'] = X_data_new['EMSGCSVERBAL'].replace({'None': 1, 'Sounds': 2, 'Words': 3,\n","                                                                     'Confused': 4, 'Oriented': 5})\n","except:\n","    pass\n","try:\n","    X_data_new['EMSGCSMOTOR'] = X_data_new['EMSGCSMOTOR'].replace({'None': 1, 'Extension': 2, 'Abnormal Flexion': 3,\n","                                                                 'Normal Flexion': 4, 'Localising': 5, 'Obeys commands': 6})\n","except:\n","    pass\n","try:\n","    X_data_new['TBIGCSMOTOR'] = X_data_new['TBIGCSMOTOR'].replace({'None': 1, 'Extension': 2, 'Abnormal Flexion': 3,\n","                                                                 'Normal Flexion': 4, 'Localising': 5, 'Obeys commands': 6})\n","except:\n","    pass\n","try:\n","    X_data_new['GCSVERBAL'] = X_data_new['GCSVERBAL'].replace({'None': 1, 'Sounds': 2, 'Words': 3,\n","                                                               'Confused': 4, 'Orientated': 5})\n","except:\n","    pass\n","try:\n","    X_data_new['GCSMOTOR'] = X_data_new['GCSMOTOR'].replace({'None': 1, 'Extension': 2, 'Abnormal Flexion': 3,\n","                                                           'Normal Flexion': 4, 'Localising': 5, 'Obeys commands': 6})\n","except:\n","    pass\n","try:\n","    X_data_new['RESPIRATORYASSISTANCE'] = X_data_new['RESPIRATORYASSISTANCE'].replace({'Assisted Respiratory Rate': 1,\n","                                                                                   'Unassisted Respiratory Rate': 0})\n","except:\n","    pass\n","try:\n","    X_data_new['SUPPLEMENTALOXYGEN'] = X_data_new['SUPPLEMENTALOXYGEN'].replace({'Supplemental Oxygen': 1,\n","                                                                             'No Supplemental Oxygen': 0})\n","except:\n","    pass\n","\n","X_data_new.head()\n","\n","##male coded as 0\n","##female coded as 1\n","\n","##not hispanic coded as 0\n","##hispanic coded as 1"]},{"cell_type":"code","execution_count":null,"id":"3f17738d","metadata":{"id":"3f17738d"},"outputs":[],"source":["##need to convert categorical values to numerical values using one-hot encoding\n","categorical_column=[]\n","for c in X_data_new.columns:\n","    if X_data_new[c].dtype == np.dtype('O', 'category'): # non-numeric values\n","        categorical_column.append(c)\n","categorical_column"]},{"cell_type":"code","execution_count":null,"id":"2770257a","metadata":{"id":"2770257a"},"outputs":[],"source":["##check how many variables we need to one-hot encode\n","len(categorical_column)"]},{"cell_type":"code","execution_count":null,"id":"60e3b4bf","metadata":{"id":"60e3b4bf"},"outputs":[],"source":["##verify dataframe shape\n","X_data_new.shape"]},{"cell_type":"code","execution_count":null,"id":"323357b1","metadata":{"id":"323357b1"},"outputs":[],"source":["##one-hot encode variables above\n","X_clean=pd.get_dummies(X_data_new, columns=categorical_column, sparse=False)\n","X_clean.shape"]},{"cell_type":"code","execution_count":null,"id":"c524ec1d","metadata":{"id":"c524ec1d"},"outputs":[],"source":["##verify cleaned true label dataframe shape\n","Y_clean.shape"]},{"cell_type":"code","execution_count":null,"id":"44d987e0","metadata":{"id":"44d987e0"},"outputs":[],"source":["##verify no missing data in the cleaned input space\n","X_clean.isnull().sum().sum()"]},{"cell_type":"code","execution_count":null,"id":"Qc1yYzaQk2T8","metadata":{"id":"Qc1yYzaQk2T8"},"outputs":[],"source":["##drop patient ID's\n","X_clean.drop(['inc_key'], axis=1, inplace=True)"]},{"cell_type":"code","execution_count":null,"id":"630434e8","metadata":{"id":"630434e8"},"outputs":[],"source":["##replace boolean values in binary variables to numeric values\n","X_clean = X_clean.replace({True: 1, False: 0})"]},{"cell_type":"code","execution_count":null,"id":"akDngVKTofJs","metadata":{"id":"akDngVKTofJs"},"outputs":[],"source":["##verify dataframe appears as intended\n","X_clean.head()"]},{"cell_type":"code","execution_count":null,"id":"AbyaxTByRK__","metadata":{"id":"AbyaxTByRK__"},"outputs":[],"source":["##split cleaned input space into training and test sets\n","X_train, X_test, Y_train, Y_test = train_test_split(X_clean, Y_clean, test_size=0.2, random_state=0)"]},{"cell_type":"code","execution_count":null,"id":"62nD6XkiRMfn","metadata":{"id":"62nD6XkiRMfn"},"outputs":[],"source":["##Before converting to Numpy arrays, we generate copies of thed data in tensor format to ensure we have access to\n","##tensor format data if needed\n","\n","X_train_tensor=X_train.copy()\n","Y_train_tensor=Y_train.copy()\n","X_test_tensor=X_test.copy()\n","Y_test_tesnor=Y_test.copy()"]},{"cell_type":"code","execution_count":null,"id":"MkSkefq4RN9D","metadata":{"id":"MkSkefq4RN9D"},"outputs":[],"source":["##convert sets to Numpy arrays:\n","X_train=X_train.values\n","Y_train=Y_train.values.reshape(-1)\n","X_test=X_test.values\n","Y_test=Y_test.values.reshape(-1)"]},{"cell_type":"code","execution_count":null,"id":"PYoFPWDoRPQW","metadata":{"id":"PYoFPWDoRPQW"},"outputs":[],"source":["##now we have X_train, Y_train, X_test, Y_test as numpy arrays\n","\n","scaler=StandardScaler()\n","#get the parameters of the transform\n","scaler.fit(X_train)\n","#normalize the features in the training set\n","X_train_s = scaler.transform(X_train)\n","#normalize the features in the test set\n","X_test_s = scaler.transform(X_test)\n","#normalize the features in the val set\n","##X_val_s = scaler.transform(X_val)\n","\n","##lets also scale the tensor copies we created\n","X_train_tensor_s = scaler.transform(X_train_tensor)\n","X_test_tensor_s = scaler.transform(X_test_tensor)"]},{"cell_type":"code","execution_count":null,"id":"E1DWiDLua903","metadata":{"id":"E1DWiDLua903"},"outputs":[],"source":["##further split the training set into a training and validation/calibration set\n","X_train_s_cal, X_val_s_cal, Y_train_cal, Y_val_cal = train_test_split(X_train_s, Y_train, test_size=0.2, random_state=0)"]},{"cell_type":"code","execution_count":null,"id":"vtemq7fdbjoE","metadata":{"id":"vtemq7fdbjoE"},"outputs":[],"source":["##now, initialize XGBoost model using parameters determined from gridsearchCV hyperparameter optimization\n","model_best_gb = xgb.XGBClassifier(random_state=0, colsample_bytree=1.0, learning_rate=0.05, max_depth=3, n_estimators=500, subsample=0.6)"]},{"cell_type":"code","execution_count":null,"id":"L9Wj3__pbnO6","metadata":{"id":"L9Wj3__pbnO6"},"outputs":[],"source":["# Get predicted probabilities for the positive class (mortality)\n","model_best_gb.fit(X_train_s_cal, Y_train_cal)\n","y_prob_gbo_mtp = model_best_gb.predict_proba(X_test_s)[:, 1]\n","\n","# Compute AUROC\n","auroc_gbo = roc_auc_score(Y_test, y_prob_gbo_mtp)\n","print(f\"AUROC on the test set: {auroc_gbo}\")"]},{"cell_type":"code","execution_count":null,"id":"A82t-nm2qKYQ","metadata":{"id":"A82t-nm2qKYQ"},"outputs":[],"source":["# Calibrate the model on the validation set\n","calibrated_model = CalibratedClassifierCV(estimator=model_best_gb, method='isotonic', cv='prefit')\n","calibrated_model.fit(X_val_s_cal, Y_val_cal)"]},{"cell_type":"code","execution_count":null,"id":"Pga633u7dr9m","metadata":{"id":"Pga633u7dr9m"},"outputs":[],"source":["##now, initialize penalized regression model with parameters based on GridsearchCV optimization and fit\n","model_best_lr=LogisticRegression(C=0.1, max_iter=100, penalty='l1', solver='saga')\n","model_best_lr.fit(X_train_s_cal, Y_train_cal)"]},{"cell_type":"code","execution_count":null,"id":"sz65_S6Hxgjp","metadata":{"id":"sz65_S6Hxgjp"},"outputs":[],"source":["##test model\n","y_pred_prob_lro = model_best_lr.predict_proba(X_test_s)[:, 1]\n","auroc_lro = roc_auc_score(Y_test, y_pred_prob_lro)\n","print(f'AUROC: {auroc_lro}')"]},{"cell_type":"code","source":["import numpy as np\n","from sklearn.metrics import roc_auc_score\n","\n","def paired_bootstrap_auc_test(\n","    y_true,\n","    predA,\n","    predB,\n","    n_boot=1000,\n","    alpha=0.05,\n","    random_state=None\n","):\n","    \"\"\"\n","    Compare two correlated AUCs (predA vs. predB) via paired bootstrapping,\n","    also returning 95% CI for each AUC individually.\n","\n","    Parameters\n","    ----------\n","    y_true : array-like of shape (n_samples,)\n","        Ground truth binary labels (0 or 1).\n","    predA : array-like of shape (n_samples,)\n","        Scores (e.g., probabilities) from model/variable A.\n","    predB : array-like of shape (n_samples,)\n","        Scores from model/variable B.\n","    n_boot : int, default=1000\n","        Number of bootstrap iterations.\n","    alpha : float, default=0.05\n","        Significance level (for the (1 - alpha) CIs).\n","    random_state : int or None\n","        Random seed for reproducibility.\n","\n","    Returns\n","    -------\n","    results : dict\n","        Dictionary containing:\n","        - aucA, aucB: AUC on the original dataset for each method.\n","        - aucA_ci_lower, aucA_ci_upper: (1 - alpha) CI for A's AUC.\n","        - aucB_ci_lower, aucB_ci_upper: (1 - alpha) CI for B's AUC.\n","        - baseline_diff: AUC(A) - AUC(B) on the full dataset (no resampling).\n","        - mean_diff: Mean difference of AUCs across bootstrap samples.\n","        - diff_ci_lower, diff_ci_upper: (1 - alpha) CI for AUC difference.\n","        - p_value: Approx. two-sided p-value for difference in AUC.\n","    \"\"\"\n","\n","    # Convert to NumPy arrays\n","    y_true = np.asarray(y_true)\n","    predA = np.asarray(predA)\n","    predB = np.asarray(predB)\n","\n","    # Basic checks\n","    assert len(y_true) == len(predA) == len(predB), \"Arrays must all have the same length.\"\n","    n = len(y_true)\n","\n","    # Compute AUC on the full (original) dataset\n","    aucA = roc_auc_score(y_true, predA)\n","    aucB = roc_auc_score(y_true, predB)\n","    baseline_diff = aucA - aucB\n","\n","    # Random generator\n","    rng = np.random.default_rng(random_state)\n","\n","    # Arrays to store bootstrapped AUCs\n","    aucAs = np.zeros(n_boot)\n","    aucBs = np.zeros(n_boot)\n","    diffs = np.zeros(n_boot)\n","\n","    for i in range(n_boot):\n","        # 1) Sample n indices with replacement\n","        sample_idx = rng.integers(0, n, size=n)\n","\n","        # 2) Create bootstrap samples\n","        y_samp = y_true[sample_idx]\n","        A_samp = predA[sample_idx]\n","        B_samp = predB[sample_idx]\n","\n","        # 3) Compute AUC for each method\n","        aucA_samp = roc_auc_score(y_samp, A_samp)\n","        aucB_samp = roc_auc_score(y_samp, B_samp)\n","\n","        # 4) Store AUCs and difference\n","        aucAs[i] = aucA_samp\n","        aucBs[i] = aucB_samp\n","        diffs[i] = aucA_samp - aucB_samp\n","\n","    # Compute individual AUC CIs\n","    aucA_ci_lower = np.percentile(aucAs, 100 * (alpha / 2))\n","    aucA_ci_upper = np.percentile(aucAs, 100 * (1 - alpha / 2))\n","\n","    aucB_ci_lower = np.percentile(aucBs, 100 * (alpha / 2))\n","    aucB_ci_upper = np.percentile(aucBs, 100 * (1 - alpha / 2))\n","\n","    # Compute difference CI\n","    diff_ci_lower = np.percentile(diffs, 100 * (alpha / 2))\n","    diff_ci_upper = np.percentile(diffs, 100 * (1 - alpha / 2))\n","\n","    # Approx. two-sided p-value by sign test on \"diffs\"\n","    n_neg = np.sum(diffs < 0)\n","    n_pos = np.sum(diffs > 0)\n","    p_val = 2.0 * min(n_neg, n_pos) / n_boot\n","    p_val = min(p_val, 1.0)\n","\n","    coverage = (1 - alpha) * 100.0  # e.g., 95.0 if alpha=0.05\n","\n","    return {\n","        \"aucA\": aucA,\n","        \"aucB\": aucB,\n","        \"aucA_ci_lower\": aucA_ci_lower,\n","        \"aucA_ci_upper\": aucA_ci_upper,\n","        \"aucB_ci_lower\": aucB_ci_lower,\n","        \"aucB_ci_upper\": aucB_ci_upper,\n","        \"baseline_diff\": baseline_diff,\n","        \"mean_diff\": np.mean(diffs),\n","        \"diff_ci_lower\": diff_ci_lower,\n","        \"diff_ci_upper\": diff_ci_upper,\n","        \"p_value\": p_val,\n","        \"coverage\": coverage\n","    }\n"],"metadata":{"id":"5DqP8Z2R0cZp"},"id":"5DqP8Z2R0cZp","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Dictionary to hold the predicted probabilities from models to compare against the main ML model\n","pairs = {\n","    \"LASSO\": y_pred_prob_lro,\n","}\n","\n","\n","# Iterate over each model in the pairs dictionary to perform paired bootstrap AUC tests\n","for var_name, var_array in pairs.items():\n","    # Perform paired bootstrap AUC comparison between ML model (predA) and comparison model (predB)\n","    results = paired_bootstrap_auc_test(\n","        y_true=Y_test,               # True class labels\n","        predA=y_prob_gbo_mtp,        # XBG model predicted probabilities\n","        predB=var_array,             # Comparison model (LASSO) predicted probabilities\n","        n_boot=2000,                 # Number of bootstrap samples for robust estimation\n","        alpha=0.05,                  # Significance level for 95% confidence interval\n","        random_state=42              # Seed for reproducibility\n","    )\n","\n","    # Format confidence interval output\n","    coverage_str = f\"{results['coverage']:.1f}%\"\n","\n","    # Print results with clear and formatted output\n","    print(f\"--- ML Model vs. {var_name} ---\")\n","    print(f\"AUC(ML) = {results['aucA']:.3f}, {coverage_str} CI: \"\n","          f\"[{results['aucA_ci_lower']:.3f}, {results['aucA_ci_upper']:.3f}]\")\n","    print(f\"AUC({var_name}) = {results['aucB']:.3f}, {coverage_str} CI: \"\n","          f\"[{results['aucB_ci_lower']:.3f}, {results['aucB_ci_upper']:.3f}]\")\n","    print(f\"AUC diff (ML - {var_name}) = {results['baseline_diff']:.4f}, {coverage_str} CI: \"\n","          f\"[{results['diff_ci_lower']:.4f}, {results['diff_ci_upper']:.4f}]\")\n","    print(f\"p-value = {results['p_value']:.4f}\\n\")"],"metadata":{"id":"T7fN8YVo0h4c"},"id":"T7fN8YVo0h4c","execution_count":null,"outputs":[]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[{"file_id":"1mqtZ3yWncnaWCc0bA3-EOMdsqTvcZRUx","timestamp":1735746306875},{"file_id":"1c7SToaTPRblOuRmJvr_r_J3ahpKcP0ru","timestamp":1735682313246},{"file_id":"1h-tN83E2ZJ-CD2vzRuhe6RWluGoYPPBe","timestamp":1735680498459},{"file_id":"1FgCD_8UY_XjBJtTWIv28zz6143Cr22JP","timestamp":1735679250179},{"file_id":"1hzJmdI07o7fmm6RO2oReUfp9AO1pjJqk","timestamp":1734983789836},{"file_id":"1XL3SGHIBPO06uNNL8pc4jbgyODiCHYmT","timestamp":1734454404403},{"file_id":"1DYwnmjwYAdIuKZSkwpMlcAF8yUXEA0x7","timestamp":1728001604398}],"gpuType":"V28"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"accelerator":"TPU"},"nbformat":4,"nbformat_minor":5}
